import tensorflow_hub as hub
import torch
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
import numpy as np

# âœ… 1. USE ë¡œë”© (ë¶„ì„ìš© ì°¸ê³ )
print("ğŸ”„ Universal Sentence Encoder ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì¤‘...")
use_model = hub.load("https://tfhub.dev/google/universal-sentence-encoder/4")
print("âœ… Universal Sentence Encoder ë¡œë”© ì™„ë£Œ!\n")

# âœ… 2. KoGPT2 ë¡œë”©
print("ğŸ”„ KoGPT2 ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë¡œë”© ì¤‘...")
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2")
model = GPT2LMHeadModel.from_pretrained("skt/kogpt2-base-v2")
model.eval()
print("âœ… KoGPT2 ëª¨ë¸ ë¡œë”© ì™„ë£Œ!\n")

# âœ… 3. ì…ë ¥ í…ìŠ¤íŠ¸
print("ğŸ“¥ ì‚¬ìš©ì í”„ë¡œí•„ ì½ëŠ” ì¤‘...")
user_profile_text = """
ì´íƒœì˜ë‹˜ì€ 26ì„¸ì˜ ì„œìš¸ì‹œ ì„œëŒ€ë¬¸êµ¬ì— ê±°ì£¼í•˜ëŠ” ì˜ì—… ì‚¬ì›ìœ¼ë¡œ, ì—ë„ˆì§€ê°€ ë„˜ì¹˜ê³  ì‚¬ëŒì„ ë§Œë‚˜ëŠ” ê²ƒì„ ì¢‹ì•„í•˜ëŠ” ì™¸í–¥ì ì¸ ì„±ê²©ì˜ ì†Œìœ ìì…ë‹ˆë‹¤. MBTIëŠ” ESFPë¡œ, í™œë™ì ì´ê³  ì‚¬êµì ì¸ ìŠ¤íƒ€ì¼ì´ë©°, ì‚¬êµ ëª¨ì„, í”¼íŠ¸ë‹ˆìŠ¤, ì¹´í˜ íƒë°©, ì—¬í–‰ ë“± ì‚¬ëŒë“¤ê³¼ í•¨ê»˜í•˜ëŠ” í™œë™ì— ê´€ì‹¬ì´ ë§ìŠµë‹ˆë‹¤.

í¡ì—°ì€ í•˜ì§€ ì•Šìœ¼ë©°, ìŒì£¼ëŠ” ê°€ë” ì¦ê¸°ëŠ” í¸ì…ë‹ˆë‹¤. ì´ìƒì ì¸ ë£¸ë©”ì´íŠ¸ë¡œëŠ” ì¹œê·¼í•˜ê³  ì™¸í–¥ì ì¸ ì„±ê²©ì˜ ì‚¬ëŒì„ ì„ í˜¸í•©ë‹ˆë‹¤.

ìƒí™œ ìŠµê´€ì„ ì‚´í´ë³´ë©´, ì‹ì‚¬ ì‹œê°„ì€ ê·œì¹™ì ì¸ í¸ì´ê³ , ì£¼ë°©ì€ ê°€ë” ì‚¬ìš©í•˜ëŠ” ì •ë„ì´ë©°, ì£¼ 1~2íšŒ ì •ë„ ìš”ë¦¬ë¥¼ í•©ë‹ˆë‹¤. ì²­ê²°ì— ìˆì–´ì„œëŠ” ì²­ì†Œë¥¼ ì£¼ 3íšŒ ì´ìƒ í•˜ë©°, ê³µìš© ê³µê°„ë„ í‰ê· ì ì¸ ìˆ˜ì¤€ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” í¸ì…ë‹ˆë‹¤.

ì†ŒìŒì— ëŒ€í•´ì„œëŠ” ë³´í†µ ìˆ˜ì¤€ì˜ ë¯¼ê°ë„ë¥¼ ê°€ì§€ê³  ìˆê³ , ì·¨ì¹¨ ì‹œì—ëŠ” ì¡°ìš©í•œ í™˜ê²½ì„ ì„ í˜¸í•©ë‹ˆë‹¤. ìŒì•…ì´ë‚˜ TVì˜ ë³¼ë¥¨ì€ ì ë‹¹í•œ ìˆ˜ì¤€ì„ ìœ ì§€í•©ë‹ˆë‹¤.

ìƒí™œ ë¦¬ë“¬ì€ ê·œì¹™ì ì´ë©°, ë³´í†µ ì˜¤ì „ 7ì‹œì— ê¸°ìƒí•˜ê³  ìì •ì¯¤ ì ìë¦¬ì— ë“­ë‹ˆë‹¤. ë°˜ë ¤ë™ë¬¼ì— ëŒ€í•´ì„œëŠ” ë¶€ë¶„ í—ˆìš© ì…ì¥ì´ë©°, íŠ¹íˆ ê³ ì–‘ì´ë¥¼ í—ˆìš©í•˜ê³ , ë°˜ë ¤ë™ë¬¼ ì•Œë ˆë¥´ê¸°ëŠ” ì—†ìŠµë‹ˆë‹¤.
"""
print("âœ… ì‚¬ìš©ì í”„ë¡œí•„ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ!\n")

# âœ… 4. USE ì„ë² ë”© (ì„ íƒì  ì°¸ê³ ìš©)
print("ğŸ“Š í”„ë¡œí•„ ì„ë² ë”© ì¤‘...")
embedding = use_model([user_profile_text])[0].numpy()
print("âœ… ì„ë² ë”© ì™„ë£Œ!\n")

# âœ… 5. ìƒê° ìƒì„± í•¨ìˆ˜
def generate_thought_with_kogpt2(text):
    print("ğŸ§  KoGPT2ë¡œ ìƒê° ìƒì„± ì¤‘...\n")
    prompt = (
        f"{text}\n\nì´ ì‚¬ìš©ìì— ëŒ€í•´ ë¶„ì„í•œ ê²°ê³¼,\nê·¸ì˜ ì„±ê²©ê³¼ ìƒí™œ ìŠµê´€ì„ ê³ ë ¤í•œ ì¡°ì–¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
    )
    input_ids = tokenizer.encode(prompt, return_tensors="pt")
    output = model.generate(
        input_ids,
        max_length=300,
        do_sample=True,
        top_p=0.92,
        top_k=50,
        temperature=0.8,
        no_repeat_ngram_size=2,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    result = tokenizer.decode(output[0], skip_special_tokens=True)
    print("âœ… ìƒì„± ì™„ë£Œ!\n")
    return result.replace(prompt, "").strip()

# âœ… 6. ê²°ê³¼ ì¶œë ¥
thought = generate_thought_with_kogpt2(user_profile_text)
print("ğŸ¤– ìƒì„±ëœ ë§ì¶¤í˜• ìƒê°:\n")
print(thought)
