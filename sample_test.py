import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def convert_to_prompt(user):
    lines = []
    lines.append(f"{user['name']}ë‹˜({user['age']}ì„¸)ì€ {user['location']}ì— ê±°ì£¼í•˜ëŠ” {user['job']}ìœ¼ë¡œ, {user['introduction']}")
    lines.append(f"MBTIëŠ” {user['mbti']}ì´ë©°, {', '.join(user['interests'])}ì„(ë¥¼) ì¦ê¹ë‹ˆë‹¤.")
    lines.append(f"{user['smoking']}ì´ê³  {user['drinking']}í•©ë‹ˆë‹¤.")
    lines.append(user['idealRoommate'])

    for section in user["lifestyle"]:
        items = [f"{item['label']}ì€ {item['value']}" for item in section["items"]]
        lines.append(" / ".join(items))

    return "\n".join(lines)

def load_model(model_path):
    print("ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto"
    )
    return tokenizer, model

def generate_summary(prompt, tokenizer, model, max_new_tokens=256):
    system_prompt = (
        "ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ìƒí™œ ìŠ¤íƒ€ì¼ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë¬¸ë‹¨ ì†Œê°œë¥¼ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.\n"
        "ë‹¤ìŒ ì¡°ê±´ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”:\n"
        "1. ì…ë ¥ëœ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ì¸¡í•˜ê±°ë‚˜ ê³¼ì¥í•˜ì§€ ë§ˆì„¸ìš”.\n"
        "2. í‘œí˜„ì€ ìì—°ìŠ¤ëŸ½ê³  ë¶€ë“œëŸ½ê²Œ ì´ì–´ì§€ë„ë¡ êµ¬ì„±í•˜ì„¸ìš”.\n"
        "3. ëª¨ë“  ì£¼ìš” ì •ë³´(ê±°ì£¼ì§€, ì§ì—…, ì„±ê²©, MBTI, ì·¨ë¯¸, ìƒí™œìŠµê´€ ë“±)ë¥¼ ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”.\n"
        "4. í•œ ë¬¸ë‹¨ ë¶„ëŸ‰ìœ¼ë¡œ ìš”ì•½í•˜ë˜, ë„ˆë¬´ ê°„ê²°í•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”.\n"
        "5. ë¶€ì •í™•í•œ í‘œí˜„(ì˜ˆ: 'ê±°ì˜ í•˜ì§€ ì•ŠìŒ', 'í—ˆìš©ë˜ì§€ ì•ŠìŒ')ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
    )

    few_shot_example = (
        "### ì…ë ¥:\n"
        "ê¹€í•˜ëŠ˜ë‹˜(29ì„¸)ì€ ë¶€ì‚° í•´ìš´ëŒ€êµ¬ì— ê±°ì£¼í•˜ëŠ” ë””ìì´ë„ˆë¡œ, ì¡°ìš©í•˜ê³  ì°¨ë¶„í•œ ì„±ê²©ì…ë‹ˆë‹¤.\n"
        "MBTIëŠ” INFJì´ë©°, ë…ì„œ, ì˜í™” ê°ìƒ, ìš”ê°€ë¥¼ ì¦ê¹ë‹ˆë‹¤.\n"
        "ë¹„í¡ì—°ìì´ë©° ìŒì£¼ëŠ” í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n"
        "ì‹ì‚¬ëŠ” ê·œì¹™ì ìœ¼ë¡œ í•˜ë©°, ì£¼ë°©ì€ ìì£¼ ì‚¬ìš©í•˜ê³  ì£¼ 3~4íšŒ ìš”ë¦¬í•©ë‹ˆë‹¤.\n"
        "ì²­ê²° ìˆ˜ì¤€ì€ ë†’ìœ¼ë©°, ì²­ì†ŒëŠ” ì£¼ 5íšŒ ì´ìƒ í•©ë‹ˆë‹¤. ê³µìš©ê³µê°„ë„ ê¹”ë”í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤.\n"
        "ì†ŒìŒì— ë¯¼ê°í•˜ë©°, ì¡°ìš©í•œ í™˜ê²½ì„ ì„ í˜¸í•©ë‹ˆë‹¤. ê¸°ìƒì€ ì˜¤ì „ 6ì‹œ, ì·¨ì¹¨ì€ ì˜¤í›„ 11ì‹œì…ë‹ˆë‹¤.\n"
        "ë°˜ë ¤ë™ë¬¼ì€ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë©°, ì•Œë ˆë¥´ê¸°ê°€ ìˆìŠµë‹ˆë‹¤.\n"
        "### ì¶œë ¥:\n"
        "ê¹€í•˜ëŠ˜ë‹˜(29ì„¸)ì€ ë¶€ì‚° í•´ìš´ëŒ€êµ¬ì— ê±°ì£¼í•˜ëŠ” ë””ìì´ë„ˆë¡œ, ì¡°ìš©í•˜ê³  ì°¨ë¶„í•œ ì„±ê²©ì„ ì§€ë‹Œ INFJ ìœ í˜•ì…ë‹ˆë‹¤. ë…ì„œ, ì˜í™” ê°ìƒ, ìš”ê°€ë¥¼ ì¦ê¸°ë©° í˜¼ìë§Œì˜ ì‹œê°„ì„ ì†Œì¤‘íˆ ì—¬ê¹ë‹ˆë‹¤. ë¹„í¡ì—°ìì´ë©° ìŒì£¼ëŠ” í•˜ì§€ ì•Šìœ¼ë©°, ì‹ì‚¬ëŠ” ê·œì¹™ì ìœ¼ë¡œ í•˜ê³  ì£¼ë°©ì„ ìì£¼ ì‚¬ìš©í•´ ì£¼ 3~4íšŒ ìš”ë¦¬í•©ë‹ˆë‹¤. ì²­ê²° ìˆ˜ì¤€ì´ ë†’ì•„ ì£¼ 5íšŒ ì´ìƒ ì²­ì†Œí•˜ë©°, ê³µìš©ê³µê°„ë„ í•­ìƒ ê¹”ë”í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤. ì†ŒìŒì— ë¯¼ê°í•´ ì¡°ìš©í•œ í™˜ê²½ì„ ì„ í˜¸í•˜ê³ , ì˜¤ì „ 6ì‹œì— ê¸°ìƒí•˜ì—¬ ì˜¤í›„ 11ì‹œì— ì·¨ì¹¨í•˜ëŠ” ê·œì¹™ì ì¸ ìƒí™œì„ ìœ ì§€í•©ë‹ˆë‹¤. ë°˜ë ¤ë™ë¬¼ì€ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë©°, ì•Œë ˆë¥´ê¸°ê°€ ìˆìŠµë‹ˆë‹¤.\n\n"
    )

    full_prompt = f"{system_prompt}\n{few_shot_example}### ì…ë ¥:\n{prompt}\n### ì¶œë ¥:\n"

    inputs = tokenizer(
        full_prompt,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=1024
    ).to(model.device)

    if "token_type_ids" in inputs:
        del inputs["token_type_ids"]

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return generated.split("### ì¶œë ¥:")[-1].strip()


def load_json(filepath):
    with open(filepath, "r", encoding="utf-8") as f:
        data = json.load(f)
    return data

def main():
    model_path = "./KoAlpaca-Polyglot-5.8B"
    user_list = load_json("user.json")

    tokenizer, model = load_model(model_path)

    for user in user_list:
        prompt = convert_to_prompt(user)
        print("\nğŸ§¾ [ì…ë ¥ í”„ë¡¬í”„íŠ¸]:\n", prompt)
        print("\nğŸ“ [ìƒì„±ëœ ì†Œê°œ ë¬¸ì¥]:\n", generate_summary(prompt, tokenizer, model))
        print("=" * 100)

if __name__ == "__main__":
    main()
