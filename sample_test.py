import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
import torch

# ëª¨ë¸ ì´ë¦„
model_path = "KoAlpaca-Polyglot-5.8B"

# í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_path)

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
)

# í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸
generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

# ì‚¬ìš©ì ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
with open("user_data.json", "r", encoding="utf-8") as f:
    users = json.load(f)

def make_prompt(user):
    prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì •ë³´ì…ë‹ˆë‹¤. ì´ ì‚¬ìš©ìì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤„ê¸€ ì„¤ëª…ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì •ë³´:
ì´ë¦„: {user['name']}
ë‚˜ì´: {user['age']}
ì§ì—…: {user['job']}
ê±°ì£¼ì§€: {user['location']}
MBTI: {user['mbti']}
ì†Œê°œ: {user['introduction']}
ê´€ì‹¬ì‚¬: {", ".join(user['interests'])}
í¡ì—° ì—¬ë¶€: {user['smoking']}
ìŒì£¼ ì—¬ë¶€: {user['drinking']}
ì´ìƒì ì¸ ë£¸ë©”ì´íŠ¸: {user['idealRoommate']}

ìƒí™œ ìŠµê´€:
"""
    for section in user["lifestyle"]:
        prompt += f"- {section['title']}\n"
        for item in section["items"]:
            prompt += f"  â€¢ {item['label']}: {item['value']}\n"
    prompt += "\nì¤„ê¸€ ìš”ì•½:"
    return prompt.strip()

# ì¤„ê¸€ ìƒì„±
for user in users:
    prompt = make_prompt(user)
    result = generator(prompt, max_new_tokens=300, do_sample=True, temperature=0.7)[0]["generated_text"]
    
    # ìš”ì•½ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    summary = result.split("ì¤„ê¸€ ìš”ì•½:")[-1].strip()
    print(f"\nğŸ“ {user['name']} ë‹˜ì— ëŒ€í•œ ìš”ì•½:\n{summary}\n")
