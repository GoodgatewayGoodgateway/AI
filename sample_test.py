import json
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers import pipeline
import torch

model_path = "KoAlpaca-Polyglot-5.8B"

tokenizer = AutoTokenizer.from_pretrained(model_path)

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    low_cpu_mem_usage=True,
    device_map="auto",
)

generator = pipeline("text-generation", model=model, tokenizer=tokenizer)

with open("user_data.json", "r", encoding="utf-8") as f:
    users = json.load(f)

def make_prompt(user):
    prompt = f"""
ë‹¹ì‹ ì€ ì•„ë˜ ì‚¬ìš©ì ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì¤„ê¸€ì´ ì•„ë‹Œ **ìˆ«ìê°€ ë§¤ê²¨ì§„ 5ê°œì˜ ë¬¸ì¥**ìœ¼ë¡œ êµ¬ì„±ëœ ì„¤ëª…ë¬¸ì„ ì‘ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.  
ì‚¬ìš©ìì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ë˜, **ìê¸°ì†Œê°œ í˜•ì‹ì´ë‚˜ ëª…ë ¹ë¬¸, ë©”íƒ€ë°œì–¸ ì—†ì´** ì¸ë¬¼ì˜ íŠ¹ì§•ê³¼ ìƒí™œìŠµê´€ì„ ë“œëŸ¬ë‚´ ì£¼ì„¸ìš”.

[ì‚¬ìš©ì ì •ë³´]
ì´ë¦„: {user['name']}
ë‚˜ì´: {user['age']}
ì§ì—…: {user['job']}
ê±°ì£¼ì§€: {user['location']}
MBTI: {user['mbti']}
ì†Œê°œ: {user['introduction']}
ê´€ì‹¬ì‚¬: {", ".join(user['interests'])}
í¡ì—° ì—¬ë¶€: {user['smoking']}
ìŒì£¼ ì—¬ë¶€: {user['drinking']}
ì´ìƒì ì¸ ë£¸ë©”ì´íŠ¸: {user['idealRoommate']}

[ìƒí™œ ìŠµê´€]
"""
    for section in user["lifestyle"]:
        prompt += f"- {section['title']}\n"
        for item in section["items"]:
            prompt += f"  â€¢ {item['label']}: {item['value']}\n"

    prompt += f"""

ğŸ“‹ {user['name']} ë‹˜ì— ëŒ€í•œ ìš”ì•½:  
ì‚¬ìš©ì ì •ë³´ ì¶œë ¥ (5ë¬¸ì¥)
"""
    return prompt.strip()

for user in users:
    prompt = make_prompt(user)
    result = generator(prompt, max_new_tokens=250, do_sample=True, temperature=0.7)[0]["generated_text"]
    
    summary = result.split("ì¤„ê¸€ ìš”ì•½:")[-1].strip()
    print(f"\nğŸ“ {user['name']} ë‹˜ì— ëŒ€í•œ ìš”ì•½:\n{summary}\n")
